{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df2 = pd.read_csv('F:/大学/第40期PRP/特征提取/1_feature_analysis/' + 'Intergated-DATASET-C' + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在导入数据：gps_20161108.csv\n",
      "已导入数据：gps_20161108.csv\n",
      "20161108 已生成WGS-84\n",
      "20161108 已生成UTM, 当前数据条数： 17848685\n",
      "20161108 已生成时空索引\n",
      "20161108 已处理空值, 当前数据条数： 0\n",
      "20161108 已生成速度\n",
      "20161108 已生成加速度\n",
      "20161108 已生成网格平均速度\n",
      "20161108 已生成网格平均加速度\n",
      "20161108 已生成网格浮动车数量\n",
      "20161108 已生成网格车速标准差\n",
      "20161108 已生成网格平均停车次数\n",
      "20161108 已整理网格特征\n",
      "20161108 处理完毕，放入list\n",
      "正在导入数据：gps_20161109.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\Working\\\\PRP\\\\Data\\\\Raw_Data\\\\gps_20161109.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2ab5b9938132>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'正在导入数据：gps_{date}.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m#导入原地理数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_data_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34mf'gps_{date}.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#注意我此处使用的是移动硬盘的地址\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'driver_ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'order_ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'timestamp'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lon'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lat'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimestamp\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m3600\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\prpPy36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    686\u001b[0m     )\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\prpPy36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\prpPy36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\prpPy36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\prpPy36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2010\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\Working\\\\PRP\\\\Data\\\\Raw_Data\\\\gps_20161109.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from utm import *\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "from osgeo import osr\n",
    "import coordTransform\n",
    "\n",
    "# 预设地址和其他全局变量\n",
    "feature_file_name = 'Intergated-DATASET-D'\n",
    "raw_data_path = 'D:\\\\Working\\\\PRP\\\\Data\\\\Raw_Data\\\\'\n",
    "feature_dst_path = 'D:\\\\Working\\\\PRP\\\\Data\\\\Processed_Data\\\\' + feature_file_name + '.csv'\n",
    "day_begin = '06:00:00'\n",
    "day_end = '23:00:00'\n",
    "\n",
    "#在此处设置时间窗(单位为3秒)和空间网格的边长(WGS84坐标系)\n",
    "time_interval = 200\n",
    "space_interval = 70\n",
    "# 滞留时间阈值，超过阈值视为无效订单\n",
    "\n",
    "# 设置时间区间 读取原数据\n",
    "# 时间区间: 减少单次的处理量\n",
    "jar = []\n",
    "\n",
    "for date in range(20161108,20161116):\n",
    "    if not os.path.exists('D:\\\\Working\\\\PRP\\\\Data\\\\Processed_Data\\\\' + f'feature_{date}.csv'):\n",
    "\n",
    "        time1 = f'{date} {day_begin}'\n",
    "        time2 = f'{date} {day_end}'\n",
    "        stamp1 = time.mktime(time.strptime(time1, '%Y%m%d %H:%M:%S'))\n",
    "        stamp2 = time.mktime(time.strptime(time2, '%Y%m%d %H:%M:%S'))\n",
    "\n",
    "        print(f'正在导入数据：gps_{date}.csv')\n",
    "        #导入原地理数据\n",
    "        df = pd.read_csv(raw_data_path+f'gps_{date}.csv', header = None) #注意我此处使用的是移动硬盘的地址\n",
    "        df.columns = ['driver_ID', 'order_ID', 'timestamp', 'lon', 'lat']\n",
    "        df.timestamp = df.timestamp + 8*3600\n",
    "        print(f'已导入数据：gps_{date}.csv')\n",
    "        ## 只取预设时间区间内的数据\n",
    "        df = df[(df['timestamp'] >= stamp1)&(df['timestamp'] < stamp2)].reset_index(drop = True)\n",
    "\n",
    "        # 将空间坐标转换为WGS-84(耗时会很长)\n",
    "        xy = df[['lon','lat']].apply(lambda x: coordTransform.gcj02_to_wgs84(x[0],x[1])[:2], axis = 1)\n",
    "        df['lon'] = [x[0] for x in xy]\n",
    "        df['lat'] = [x[1] for x in xy]\n",
    "        print (f'{date} 已生成WGS-84')\n",
    "\n",
    "        # 再把WGS-84转换为UTM平面直角系(保留WGS-84数据)\n",
    "        wgs84 = osr.SpatialReference()\n",
    "        wgs84.ImportFromEPSG(4326)\n",
    "        # 2.Pseudo-Mercator\n",
    "        inp = osr.SpatialReference()\n",
    "        inp.ImportFromEPSG(3857)\n",
    "        # 3.定义坐标变换映射\n",
    "        transformation = osr.CoordinateTransformation(wgs84, inp)\n",
    "        # 4.转换原数据的坐标\n",
    "        xy = df[['lon','lat']].apply(lambda x: transformation.TransformPoint(x[0],x[1])[:2], axis = 1)\n",
    "        # 5.写入df\n",
    "        df['x'] = [x[0] for x in xy]\n",
    "        df['y'] = [x[1] for x in xy]\n",
    "        print (f'{date} 已生成UTM, 当前数据条数：',len(df))\n",
    "\n",
    "        # 时间窗划分\n",
    "        df['time_id'] = df.timestamp.apply(lambda x: (x - stamp1)//time_interval)\n",
    "\n",
    "        # 空间网格划分\n",
    "        # 1.计算左边界和上边界，左右-x， 上下-y\n",
    "        left = df['x'].min()\n",
    "        down = df['y'].max()\n",
    "\n",
    "        # 2.生成横向和纵向索引\n",
    "        df['row_id'] = df['y'].apply(lambda y: (y - down)//space_interval)\n",
    "        df['col_id'] = df['x'].apply(lambda x: (x - left)//space_interval)\n",
    "\n",
    "        print (f'{date} 已生成时空索引')\n",
    "\n",
    "        df = df.dropna()\n",
    "        print (f'{date} 已处理空值, 当前数据条数：',len(df))\n",
    "\n",
    "        # 下面开始时空特征提取\n",
    "\n",
    "        #1. 计算瞬时速度\n",
    "        # 排序：先按司机排，同司机按订单排，同订单再按时间排\n",
    "        df = df.sort_values(by = ['driver_ID', 'order_ID', 'timestamp']).reset_index(drop = True)\n",
    "        # 将订单id下移一行，用于判断前后数据是否属于同一订单\n",
    "        df['orderFlag'] = df['order_ID'].shift(1)\n",
    "        df['identi'] = (df['orderFlag'] == df['order_ID']) #一个由boolean构成的列，方便后面所有shift完成了之后再删除分界行\n",
    "        # 将坐标，时间戳下移一行，匹配相应轨迹点\n",
    "        df['x1'] = df['x'].shift(1)\n",
    "        df['y1'] = df['y'].shift(1)\n",
    "        df['timestamp1'] = df['timestamp'].shift(1)\n",
    "        # 将不属于同一订单的轨迹点删除\n",
    "        df = df[df['identi'] == True]\n",
    "        # 计算相邻轨迹点之间的距离和相差时间\n",
    "        # 距离采用欧式距离\n",
    "        dist = np.sqrt(np.square(df['x'].values - df['x1'].values) + np.square(df['y'].values - df['y1'].values))\n",
    "        ttime = df['timestamp'].values - df['timestamp1'].values\n",
    "        # 计算速度\n",
    "        df['speed'] = dist/ttime\n",
    "        # 删除临时数据\n",
    "        df = df.drop(columns = ['x1', 'y1', 'orderFlag', 'timestamp1', 'identi'])\n",
    "        print(f'{date} 已生成速度')\n",
    "\n",
    "        # 2.计算瞬时加速度\n",
    "        df['speed1'] = df['speed'].shift(1)\n",
    "        df['timestamp1'] = df['timestamp'].shift(1)\n",
    "        df['identi'] = df['order_ID'].shift(1)\n",
    "        df = df[df.identi == df.order_ID]\n",
    "        df['acc'] = (df.speed - df.speed1)/(df.timestamp - df.timestamp1)\n",
    "        df = df.drop(columns = ['speed1', 'timestamp1', 'identi'])\n",
    "        print(f'{date} 已生成加速度')\n",
    "\n",
    "        df = df.reset_index(drop = True)\n",
    "\n",
    "        # 下面计算集体/网格平均特征\n",
    "\n",
    "        # 1. 网格平均速度：先求每辆车在网格中的平均速度，然后求网格中所有个体平均速度的军制\n",
    "        # 基于时空网格和估计id分组\n",
    "        orderGrouped = df.groupby(['row_id', 'col_id', 'time_id', 'order_ID'])\n",
    "        # 网格在每个时刻（时间窗）的平均速度\n",
    "        grouped_speed = orderGrouped.speed.mean().reset_index()\n",
    "        grouped_speed = grouped_speed.groupby(['row_id', 'col_id', 'time_id'])\n",
    "        grid_speed = grouped_speed.speed.mean()\n",
    "        # 去除异常值\n",
    "        grid_speed = grid_speed.clip(grid_speed.quantile(0.05), grid_speed.quantile(0.95))\n",
    "        print(f'{date} 已生成网格平均速度')\n",
    "\n",
    "        # 2. 网格平均加速度\n",
    "        gridGrouped = df.groupby(['row_id', 'col_id', 'time_id'])\n",
    "        grid_acc = gridGrouped.acc.mean()\n",
    "        print(f'{date} 已生成网格平均加速度')\n",
    "\n",
    "        # 3.网格浮动车流量\n",
    "        grouped_volume = orderGrouped.speed.last().reset_index() #每个时空网格中的每个order只保留一辆（用last（）来取）\n",
    "        grouped_volume = grouped_volume.groupby(['row_id', 'col_id', 'time_id'])\n",
    "        grid_volume = grouped_volume['speed'].size()\n",
    "        grid_volume = grid_volume.clip(grid_volume.quantile(0.05), grid_volume.quantile(0.95))\n",
    "        print(f'{date} 已生成网格浮动车数量')\n",
    "\n",
    "        # 4.网格车速标准差\n",
    "        grid_v_std = gridGrouped.speed.std(ddof=0)\n",
    "        # 去除异常值\n",
    "        grid_v_std = grid_v_std.clip(grid_v_std.quantile(0.05), grid_v_std.quantile(0.95))\n",
    "        print(f'{date} 已生成网格车速标准差')\n",
    "\n",
    "        # 5.网格平均停车次数\n",
    "        stopNum = gridGrouped.speed.agg(lambda x: (x==0).sum())\n",
    "        grid_stop = pd.concat((stopNum, grid_volume), axis = 1)\n",
    "        grid_stop['stopNum'] = stopNum.values/ grid_volume.values\n",
    "        grid_stop = grid_stop['stopNum']\n",
    "        grid_stop = grid_stop.clip(0, grid_stop.quantile(0.95))\n",
    "        print(f'{date} 已生成网格平均停车次数')\n",
    "\n",
    "        feature = pd.concat([grid_speed, grid_acc, grid_volume, grid_v_std, grid_stop], axis = 1).reset_index()\n",
    "        feature.columns = ['row_id','col_id', 'time_id', 'aveSpeed', 'gridAcc', 'volume', 'speedStd', 'stopNum']\n",
    "        print(f'{date} 已整理网格特征')\n",
    "        feature.sort_values(['stopNum']).reset_index(drop=True)\n",
    "        feature['date'] = date\n",
    "\n",
    "        jar.append(feature)\n",
    "        feature.to_csv('D:\\\\Working\\\\PRP\\\\Data\\\\Processed_Data\\\\' + f'feature_{date}.csv')\n",
    "        print(f'{date} 处理完毕，放入list')\n",
    "    \n",
    "    else:\n",
    "        feature = df.read_csv('D:\\\\Working\\\\PRP\\\\Data\\\\Processed_Data\\\\' + f'feature_{date}.csv')\n",
    "        print(f'feature_{date} 处理完毕，放入list')\n",
    "\n",
    "integrated_feature = pd.concat(jar, axis=0)\n",
    "integrated_feature.to_csv(feature_dst_path, index = None)\n",
    "print(f'已生成{feature_file_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b4b4feff2f24a0f0a34464dbe537a36fda679851528fb8735cb41fa49dffb2d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
