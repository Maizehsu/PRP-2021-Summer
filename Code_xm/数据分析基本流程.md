## 提出问题



#### 数据清洗 (Data Cleaning)

#### 理解数据 (EDA: Exploring Data Analysis)

- Structure -- the "shape" of a data file
  - Prefer rectangular data: Tables and matrix
  - Are the data in a standard format or encoding?
    - Tabular data: CSV, TSV, Excel, SQL
    - Nested data: JSON or XML
  - Are the data organized in “records”?
    - No: Can we define records by parsing the data?
  - Are the data nested? (records contained within records…)
    - Yes: Can we reasonably un-nest the data?
  - Does the data reference other data?
    - Yes: can we join/merge the data
  - What are the fields in each record?
    - How are they encoded? (e.g., strings, numbers, binary, dates …)
    - What is the type of the data?
- Granularity -- how fine/coarse is each datum
  - What does each record represent?
    - Examples: a purchase, a person, a group of users
  - Do all records capture granularity at the same level?
    - Some data will include summaries (aka rollups) as records
  - If the data are coarse how was it aggregated?
    - Sampling, averaging, …
- Scope -- how (in)complete is the data
  - Does my data cover my area of interest?
    - Example: I am interested in studying crime in Shanghai but I only have Minhang
      District data.
  - Is my data too expansive?
      - Example: I am interested in student grades for ECE 4710J but have student grades for
        all classes.
      - Solution: Filtering ⇒ Implications on sample?
      - If the data is a sample I may have poor coverage after filtering …
  - Does my data cover the right time frame?
      - More on this in temporality …
- Temporality -- how is the data situated in time
  - Data changes – when was the data collected?
  - What is the meaning of the time and date fields?
    - When the “event” happened?
    - When the data was collected or was entered into the system?
    - Date the data was copied into a database (look for many matching timestamps)
  - Time depends on where! (Time zones)
    - Learn to use datetime python library
    - Multiple string representation (depends on region): 07/08/09?
  - Are there strange null values?
    - January 1st 1970 (Unix Epoch), January 1st 1900
  - Is there periodicity? Diurnal patterns
- Faithfulness -- how well does the data capture "reality"
  - Does my data contain unrealistic or “incorrect” values?
    - Dates in the future for events in the past
    - Locations that don’t exist
    - Negative counts
    - Misspellings of names
    - Large outliers
  - Does my data violate obvious dependencies?
    - E.g., age and birthday don’t match
  - Was the data entered by hand?
    - Spelling errors, fields shifted …
    - Did the form require fields or provide default values?
  - Are there obvious signs of data falsification:
    - Repeated names, fake looking email addresses, repeated use of
      uncommon names or fields.
- Summary: How do you do EDA/Data Cleaning?
  - Examine data and metadata:
    - What is the date, size, organization, and structure of the data?
  - Examine each field/attribute/dimension individually
  - Examine pairs of related dimensions
    - Stratifying earlier analysis: break down grades by major …
  - Along the way:
    - Visualize/summarize the data
    - Validate assumptions about data and collection process
    - Identify and address anomalies
    - Apply data transformations and corrections
    - Record everything you do!

#### 建立模型

#### 数据可视化

