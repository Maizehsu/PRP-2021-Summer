{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c07ab4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-d [DATA]] [-v]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Maize\\AppData\\Roaming\\jupyter\\runtime\\kernel-b9bc8b83-3361-4c7c-a407-8701a50a471f.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\prpPy36\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description=\"《交通大数据：理论与方法》样例数据生成。\")\n",
    "parser.add_argument('-d', '--data', nargs='?', default=os.getcwd(),\n",
    "                    type=str, help=\"数据存储路径。\")\n",
    "parser.add_argument('-v', '--version', action='version', \n",
    "                    version='1.0', help=\"版本信息。\")\n",
    "_args = parser.parse_args()\n",
    "\n",
    "data_path = Path(_args.data)\n",
    "\n",
    "def extract_features(date):\n",
    "    name = 'A'\n",
    "\n",
    "    if not os.path.exists(f'{name}_ori_2016{date}.csv'):\n",
    "        raw_gps = data_path / f'gps_2016{date}'\n",
    "        if not os.path.exists(raw_gps):\n",
    "            print(f\"未找到{raw_gps.as_posix()}\")\n",
    "            return\n",
    "        df = pd.read_csv(\n",
    "            raw_gps, names=['driverID', 'orderID', 'timestamp', 'lon', 'lat'])\n",
    "\n",
    "        \"\"\"\n",
    "        数据压缩，选择研究区域数据，同时删除错误数据\n",
    "        如经纬度明显异常的数据\n",
    "        \"\"\"\n",
    "        left_geo = 104.081083   #左边界\n",
    "        right_geo = 104.111122  #右边界\n",
    "        up_geo = 30.674219      #上边界\n",
    "        down_geo = 30.653965    #下边界\n",
    "\n",
    "        df_1 = df[(df.lon>left_geo) & (df.lon<right_geo)\n",
    "                & (df.lat>down_geo) & (df.lat<up_geo)]\n",
    "        df_1.to_csv('%s_ori_2016%d.csv' %(name, date), index=None) #存储为文件\n",
    "\n",
    "        del df\n",
    "        df = df_1\n",
    "    else:\n",
    "        df = pd.read_csv('%s_ori_2016%d.csv' % (name, date))\n",
    "\n",
    "    if not os.path.exists(f'{name}-速度加速度计算-{date}.csv'):\n",
    "        \"\"\"\n",
    "        去除空值\n",
    "        \"\"\"\n",
    "        print('去除前数据条数：', len(df))\n",
    "        df = df.dropna()\n",
    "        print('去除后数据条数：', len(df))\n",
    "\n",
    "        \"\"\"\n",
    "        去除重复数据\n",
    "        \"\"\"\n",
    "        print('去除前数据条数：', len(df))\n",
    "        df = df.drop_duplicates()\n",
    "        print('去除后数据条数：', len(df))\n",
    "\n",
    "        \"\"\"\n",
    "        坐标系转换\n",
    "        \"\"\"\n",
    "        from osgeo import osr\n",
    "\n",
    "        wgs84 = osr.SpatialReference()\n",
    "        wgs84.ImportFromEPSG(4326)  #wgs-84坐标系\n",
    "        inp = osr.SpatialReference()\n",
    "        inp.ImportFromEPSG(3857)    #Pseudo-Mercator坐标系\n",
    "        # 定义坐标转换\n",
    "        transformation = osr.CoordinateTransformation(wgs84, inp)\n",
    "\n",
    "        #转换坐标\n",
    "        xy = df[['lon', 'lat']].apply(\n",
    "            lambda x: transformation.TransformPoint(x[1], x[0])[:2], axis=1)\n",
    "\n",
    "        # xy为一个list，每一个元素为一个tuple\n",
    "        # 转换为dataframe中的两列\n",
    "        df['x'] = [x[0] for x in xy]\n",
    "        df['y'] = [x[1] for x in xy]\n",
    "\n",
    "        \"\"\"\n",
    "        离散化\n",
    "        \"\"\"\n",
    "        # 转换为utc+8时区\n",
    "        df.timestamp = df.timestamp + 8 * 3600\n",
    "        # currDay为当日0时的timestamp\n",
    "        currDay = pd.Timestamp('2016%d' % date).timestamp()\n",
    "        df['time_id'] = (df.timestamp.values - currDay) // 600\n",
    "\n",
    "        # 空间网格计算\n",
    "        left = df.x.min()   # 网格左边界\n",
    "        down = df.y.min()   # 网格下边界\n",
    "        unit = 50           # 定义网格大小\n",
    "        df['rowid'] = (df.y.values - down) // unit\n",
    "        df['colid'] = (df.x.values - left) // unit\n",
    "\n",
    "        \"\"\"\n",
    "        交通流参数计算\n",
    "        \"\"\"\n",
    "        df = df.sort_values(\n",
    "            by=['driverID', 'orderID', 'timestamp']).reset_index(drop=True)\n",
    "        # 将订单id，下移一行，用于判断相邻记录是否属于同一订单\n",
    "        df['orderFlag'] = df['orderID'].shift(1)\n",
    "        df['identi'] = (df['orderFlag']==df['orderID'])\n",
    "        # 将坐标、时间戳下移一行，从而匹配相邻轨迹点\n",
    "        df['x1'] = df['x'].shift(1)\n",
    "        df['y1'] = df['y'].shift(1)\n",
    "        df['timestamp1'] = df['timestamp'].shift(1)\n",
    "        # 将不属于同一订单的轨迹点对删去\n",
    "        df = df[df['identi']==True]\n",
    "\n",
    "        # 计算距离\n",
    "        dist = np.sqrt(np.square(\n",
    "            (df['x'].values-df['x1'].values)) + np.square(\n",
    "                (df['y'].values-df['y1'].values)))\n",
    "        # 计算时间\n",
    "        time = df['timestamp'].values - df['timestamp1'].values\n",
    "        # 计算速度\n",
    "        df['speed'] = dist / time\n",
    "        # 删去无用列\n",
    "        df = df.drop(columns=['x1', 'y1', 'orderFlag', 'timestamp1', 'identi'])\n",
    "\n",
    "        # 计算加速度\n",
    "        df['speed1'] = df.speed.shift(1)\n",
    "        df['timestamp1'] = df.timestamp.shift(1)\n",
    "        df['identi'] = df.orderID.shift(1)\n",
    "        df = df[df.orderID==df.identi]\n",
    "\n",
    "        df.loc[:, 'acc'] = (df.speed1.values - df.speed.values) / (\n",
    "            df.timestamp1.values - df.timestamp.values)\n",
    "        df = df.drop(columns=['speed1', 'timestamp1', 'identi'])\n",
    "\n",
    "        df.to_csv(f'{name}-速度加速度计算-{date}.csv', index=None)\n",
    "    else:\n",
    "        df = pd.read_csv(f'{name}-速度加速度计算-{date}.csv')\n",
    "\n",
    "    if not os.path.exists('featureNew-%d.csv' % date):\n",
    "        \"\"\"\n",
    "        计算网格交通流参数\n",
    "        \"\"\"\n",
    "        orderGrouped = df.groupby(['rowid', 'colid', 'orderID', 'time_id'])\n",
    "        gridGrouped = df.groupby(['rowid', 'colid', 'time_id'])\n",
    "\n",
    "        # 网格平均车速\n",
    "        grouped_speed = orderGrouped.speed.mean().reset_index()\n",
    "        grouped_speed = grouped_speed.groupby(['rowid', 'colid', 'time_id'])\n",
    "        grid_speed = grouped_speed.speed.mean()\n",
    "        grid_speed = grid_speed.clip(\n",
    "            grid_speed.quantile(0.05), grid_speed.quantile(0.95))\n",
    "\n",
    "        # 网格平均加速度\n",
    "        grid_acc = gridGrouped.acc.mean()\n",
    "\n",
    "        # 网格流量\n",
    "        grouped_volume = orderGrouped.speed.last().reset_index()\n",
    "        grouped_volume = grouped_volume.groupby(['rowid', 'colid', 'time_id'])\n",
    "        grid_volume = grouped_volume['speed'].size()\n",
    "        grid_volume = grid_volume.clip(\n",
    "            grid_volume.quantile(0.05), grid_volume.quantile(0.95))\n",
    "\n",
    "        # 网格车速标准差\n",
    "        grid_v_std = gridGrouped.speed.std()\n",
    "\n",
    "        # 网格平均停车次数\n",
    "        stopNum = gridGrouped.speed.agg(lambda x: (x==0).sum())\n",
    "        grid_stop = pd.concat((stopNum, grid_volume), axis=1)\n",
    "        grid_stop['stopNum'] = stopNum.values / grid_volume.values\n",
    "        grid_stop = grid_stop['stopNum']\n",
    "        grid_stop = grid_stop.clip(0, grid_stop.quantile(0.95))\n",
    "\n",
    "        # 网格最小车速\n",
    "        t = orderGrouped['timestamp'].last() - orderGrouped['timestamp'].first()\n",
    "        dist = np.sqrt(\n",
    "            (orderGrouped['x'].last().values - orderGrouped['x'].first().values)**2 + \\\n",
    "            (orderGrouped['y'].last().values - orderGrouped['y'].first().values) ** 2)\n",
    "        grid_min_speed = t.reset_index()\n",
    "        grid_min_speed['minSpeed'] = dist / t.values\n",
    "        grid_min_speed = grid_min_speed.groupby(\n",
    "            ['rowid', 'colid', 'time_id']).minSpeed.mean()\n",
    "\n",
    "        # 网格自由流车速\n",
    "        grid_free_speed = df.groupby(\n",
    "            ['rowid', 'colid'], as_index=False).speed.max()\n",
    "        grid_free_speed.columns = ['rowid', 'colid', 'freeSpeed']\n",
    "\n",
    "        feature = pd.concat([\n",
    "            grid_speed, grid_acc, grid_volume, grid_v_std, \n",
    "            grid_stop, grid_min_speed], axis=1).reset_index()\n",
    "        feature.columns = [\n",
    "            'rowid', 'colid', 'time_id', 'aveSpeed', \n",
    "            'gridAcc', 'volume', 'speed_std', 'stopNum', 'minSpeed']\n",
    "        feature = pd.merge(\n",
    "            feature, grid_free_speed, how='left', on=['rowid', 'colid'])\n",
    "        feature['speedRatio'] = feature.minSpeed / feature.freeSpeed\n",
    "        feature['minSpeed'].fillna(0, inplace=True)\n",
    "        feature['speedRatio'].fillna(0, inplace=True)\n",
    "        feature.to_csv('featureNew-%d.csv' % date, index=None)\n",
    "    \n",
    "\n",
    "def do_kmeans(n_clusters, data):\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=n_clusters, random_state=0, algorithm='auto').fit(data)\n",
    "    return kmeans.labels_  \n",
    "\n",
    "\n",
    "def cluster_ana(cluster_method='kmeans'):\n",
    "    data_ori = pd.read_csv('train-v20.csv')\n",
    "    data_ori = data_ori.dropna()\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(data_ori[['aveSpeed', 'gridAcc', 'speed_std', 'stopNum']])\n",
    "    data_ori_nor = scaler.transform(\n",
    "        data_ori[['aveSpeed', 'gridAcc', 'speed_std', 'stopNum']])\n",
    "    data_ori_nor = pd.DataFrame(\n",
    "        data_ori_nor, columns=['aveSpeed', 'gridAcc', 'speed_std', 'stopNum'])\n",
    "    data = data_ori_nor.values\n",
    "    \n",
    "    methods = {'kmeans': do_kmeans}\n",
    "    n_clusters = 3\n",
    "    labels = methods[cluster_method](n_clusters, data)\n",
    "    \n",
    "    data_ori['labels'] = labels\n",
    "    data_ori.to_csv('DATASET-B.csv', index=None)\n",
    "    \n",
    "    \n",
    "def generate_labels():\n",
    "    jar = []\n",
    "    for dt in range(1101, 1131):\n",
    "        feature = 'featureNew-%d.csv' % dt\n",
    "        if os.path.exists(feature):\n",
    "            df = pd.read_csv(feature)\n",
    "            df['date'] = '2016' + str(dt)\n",
    "            jar.append(df)\n",
    "        else:\n",
    "            print(f\"未找到'{feature}'。\")\n",
    "            \n",
    "    df = pd.concat(jar, axis=0)\n",
    "    df.to_csv('train-v20.csv', index=None)\n",
    "    cluster_ana()\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    for date in range(1101, 1131):\n",
    "        extract_features(date)\n",
    "    generate_labels()\n",
    "    print(\"DATASET-B生成完毕！\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb26ac2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:prpPy36]",
   "language": "python",
   "name": "conda-env-prpPy36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
